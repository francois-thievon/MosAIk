# MOSAIK â€” Comparaison de la qualitÃ© de modÃ©lisation de lâ€™incertitude dâ€™un auto-encodeur vs. un modÃ¨le de classification

## ğŸ¯ Objectif du projet

Ce projet de recherche vise Ã  **vÃ©rifier lâ€™hypothÃ¨se de perte dâ€™information concernant la modÃ©lisation de lâ€™incertitude** lorsque lâ€™on contraint un modÃ¨le Ã  prÃ©dire une Ã©tiquette.  
Autrement dit, lâ€™Ã©tude cherche Ã  montrer que **lâ€™apprentissage supervisÃ© multi-classes conduit Ã  une dÃ©gradation de la qualitÃ© de la modÃ©lisation de lâ€™incertitude**, par rapport Ã  un auto-encodeur non contraint par une tÃ¢che de classification.

Le travail sâ€™inscrit dans le cadre du **projet MOSAIK (ModÃ©lisation et Quantification dâ€™Incertitude)**, menÃ© au sein de lâ€™Ã©quipe **LORIA â€“ Ã©quipe MosAIk**, et sâ€™appuie sur les mÃ©thodes rÃ©centes de **quantification dâ€™incertitude**.

---

## ğŸ§  Contexte scientifique

Un modÃ¨le de classification probabiliste classique ne capture quâ€™un seul type dâ€™incertitude : la **probabilitÃ© dâ€™appartenance Ã  une classe**.  
Or, plusieurs natures dâ€™incertitude coexistent :

- **Incertitude alÃ©atoire (Ã©pistÃ©mique)** : liÃ©e Ã  la difficultÃ© intrinsÃ¨que du problÃ¨me (ex. lancer de piÃ¨ce).
- **Incertitude informationnelle (modÃ©lisation)** : liÃ©e Ã  un manque de donnÃ©es ou Ã  des observations rares (ex. apparition dâ€™un O.V.N.I dans un problÃ¨me Avion/Oiseau).

Plusieurs approches existent pour modÃ©liser ces diffÃ©rentes incertitudes :
- **MÃ©thodes bayÃ©siennes** (ex. Dropout bayÃ©sien, BNN)
- **MÃ©thodes ensemblistes** (ensembles de modÃ¨les, variance prÃ©dictive)
- **Minimisation de risque de second ordre**
- **MÃ©thodes par densitÃ© locale**

Le projet explore cette problÃ©matique Ã  travers trois approches complÃ©mentaires : dÃ©monstration, preuve empirique, et preuve formelle.

---

## ğŸ—‚ï¸ Structure du dÃ©pÃ´t

```bash
MOSAIK/
â”œâ”€â”€ demonstration/              # Application sur donnÃ©es rÃ©elles (MNIST, CIFAR-10)
â”‚   â”œâ”€â”€ notebooks/              # Notebooks de visualisation et dâ€™analyse
â”‚   â”œâ”€â”€ models/                 # RÃ©seaux LeNet, auto-encodeur, etc.
â”‚   â”œâ”€â”€ results/                # Figures et mÃ©triques gÃ©nÃ©rÃ©es
â”‚   â””â”€â”€ README.md               # Description de cette partie
â”‚
â”œâ”€â”€ preuve_empirique/           # ExpÃ©rimentations sur donnÃ©es simulÃ©es
â”‚   â”œâ”€â”€ simulations/            # GÃ©nÃ©ration manuelle de jeux de donnÃ©es
â”‚   â”œâ”€â”€ analyses/               # Ã‰tudes statistiques et convergence (loi des grands nombres)
â”‚   â”œâ”€â”€ plots/                  # Visualisations des rÃ©sultats empiriques
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ preuve_formelle/            # Partie thÃ©orique et dÃ©monstrations mathÃ©matiques
â”‚   â”œâ”€â”€ pdf/                    # Documents LaTeX / notes de preuve
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ utils/                      # Fonctions communes : visualisation, mÃ©triques, incertitude
â”œâ”€â”€ requirements.txt            # DÃ©pendances Python
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md                   # Ce fichier
```
---

## ğŸ”¬ Approches complÃ©mentaires

### 1. **DÃ©monstration (Application sur donnÃ©es rÃ©elles)**

Cette premiÃ¨re approche illustre la problÃ©matique sur des jeux de donnÃ©es standards :
- **MNIST** (chiffres manuscrits) pour lâ€™expÃ©rimentation initiale
- **CIFAR-10** (images couleur) pour des tests plus complexes

Lâ€™objectif est de **visualiser les diffÃ©rences de comportement entre un modÃ¨le de classification et un auto-encodeur** en termes dâ€™incertitude :
- Distribution des probabilitÃ©s de sortie
- Mesure de confiance (entropy, mutual information, variance prÃ©dictive)
- ReprÃ©sentation des embeddings latents

---

### 2. **Preuve empirique (Validation statistique sur donnÃ©es simulÃ©es)**

Cette partie vise Ã  **confirmer empiriquement** les observations faites sur les donnÃ©es rÃ©elles :
- GÃ©nÃ©ration manuelle de jeux de donnÃ©es simples (2D, classes sÃ©parables ou ambiguÃ«s)
- Application rÃ©pÃ©tÃ©e de modÃ¨les pour observer la **stabilitÃ© et la convergence** des mesures dâ€™incertitude
- Ã‰tude de la **loi des grands nombres** appliquÃ©e aux estimations de confiance

Lâ€™objectif est de montrer que la perte dâ€™information se vÃ©rifie de maniÃ¨re robuste sur de nombreux tirages indÃ©pendants.

---

### 3. **Preuve formelle (DÃ©monstration mathÃ©matique)**

Enfin, une analyse thÃ©orique vient **Ã©tablir formellement** les bases de lâ€™hypothÃ¨se :
- DÃ©finition mathÃ©matique de lâ€™incertitude (entropie, divergence de Kullback-Leibler, etc.)
- Analyse de la sur-spÃ©cification du modÃ¨le de classification
- Mise en Ã©vidence de la perte dâ€™information par projection sur lâ€™espace des classes

Cette section ne contient **aucun code**, uniquement des dÃ©monstrations et dÃ©veloppements analytiques.

---

## ğŸ§° Technologies principales

- **Python 3.10+**
- **PyTorch / TensorFlow** pour les modÃ¨les
- **NumPy / SciPy / Pandas** pour le traitement des donnÃ©es
- **Matplotlib / Seaborn / Plotly** pour la visualisation
- **Jupyter Notebook** pour la documentation interactive

---

## ğŸ“ˆ Plan du projet

1. EntraÃ®ner un modÃ¨le **LeNet** sur MNIST.  
2. Transformer le rÃ©seau en **auto-encodeur**.  
3. ImplÃ©menter diffÃ©rentes **mÃ©thodes de quantification dâ€™incertitude**.  
4. DÃ©finir un **critÃ¨re de comparaison** robuste de la qualitÃ© de la modÃ©lisation dâ€™incertitude.  
5. Ã‰valuer et comparer les modÃ¨les.  
6. (Optionnel) Ã‰tendre lâ€™Ã©tude Ã  **CIFAR-10** et des architectures plus profondes.  

---

## ğŸ‘¥ Encadrement

Projet de recherche menÃ© au sein de lâ€™Ã©quipe **MosAIk â€“ LORIA**, dans le cadre dâ€™un **projet de fin dâ€™Ã©tudes** en data science et intelligence artificielle.

---

## ğŸ“œ Licence

Ce projet est distribuÃ© sous licence MIT â€” voir le fichier [LICENSE](./LICENSE) pour plus dâ€™informations.

---

